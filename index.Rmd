---
title: "SDS/CSC 293: Machine Learning"
author: "Albert Y. Kim"
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
#    df_print: kable
---

<style>
h1{font-weight: 400;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE, 
                      cache=TRUE, fig.width=16/2, fig.height=9/2)
library(tidyverse)
library(broom)
library(knitr)
library(modelr)
library(lubridate)
library(forcats)
library(nycflights13)
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
library(okcupiddata)
# devtools::install_github("hadley/emo")
library(emo)

# Set seed value of random number generator to get "replicable" random numbers.
# Why 76? Because of https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

```{r, eval=FALSE, echo=FALSE}
# Run this separately to have slide output:
rmarkdown::render("index.Rmd", output_format = c("ioslides_presentation"), output_file = "slides.html")
```

<style>
h1{font-weight: 400;}
</style>



***



# Schedule 

<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vToPV6gfDlIq5ni1qezMcPy3ZdyIN1MtSSKZ3GABeBvf9LYu3_1XE7DOEQh4Dg02bKG5YF0XpSFO_-B/pubhtml?gid=1068367544&amp;single=true&amp;widget=true&amp;headers=false" width="100%" height="620"></iframe>

<!--
{target="_blank"}
-->



***

<!--
# Lec 11: Wed 3/6

## Announcements

* Spinelli Center tutoring hours change tomorrow (Thursday 3/7) only: 4:30-6pm and not ~~7-9pm~~.

## Todays Topics/Activities  

### 1. Chalk Talk






***
-->




# Lec 10: Mon 3/4

## Announcements

* Reminder tomorrow is: 

<center>
![](static/events/emma_benn_2.png){ width=650px }
<center> 

<br>

## Todays Topics/Activities  

### 1. Chalk Talk

* Intro to logistic regression
* Transforming probability space to log-odds/logit space. The following two plots are the same but with the axes flipped. In other words:
    + The left plot visualizes how we map from probability space $[0,1]$ to log-odds space $(-\infty, \infty)$
    + The right plot visualizes how we map from log-odds space $(-\infty, \infty)$ back to probability space $[0,1]$

<!--
Here are the two equations:

$$
\begin{align}
\mbox{left function} = \mbox{log-odds} = \mbox{logit}(p) &= \log\left(\frac{p}{1-p}\right) = \alpha\\
\mbox{right function} = \mbox{inverse-logit}(\alpha) &= \frac{\exp(\alpha)}{\exp(\alpha) + 1} = \frac{1}{1+ \exp(-\alpha)} = p
\end{align}
$$
--> 


```{r, echo=FALSE, eval=TRUE}
eps <- 0.0001
values <- data_frame(
  p = seq(from = eps, to = 1-eps, length = 1000),
  y = log(p/(1-p))
)
points <- data_frame(
  p = c(0.01, 1-0.01, 0.5),
  y = log(p/(1-p)),
  group = factor(1:3)
)

plot1 <- ggplot(values, aes(p, y)) +
  geom_line() +
  geom_hline(yintercept = 0,  alpha=0.5) +
  geom_vline(xintercept = 0.5, alpha=0.5) +
  # geom_vline(xintercept = 0, linetype="dashed", alpha=0.5) +
  # geom_vline(xintercept = 1, linetype="dashed", alpha=0.5) +
  geom_point(data=points, aes(col=group), size=5) + 
  theme(legend.position="none") +
  labs(x="Probability p", y="log-odds", title="log-odds over probability")
plot2 <- plot1 +
  coord_flip() + 
  labs(x="Probability p", y="log-odds", title="probability over log-odds")
plot1 + plot2
```



### 2. In-class exercise

* Work on MP2



***



# Lec 9: Wed 2/27

## Announcements

* As indicated in the [syllabus](syllabus.html#policies), even if you are done any in-class exercises early, please remain until the end of lecture. It is very disruptive to the class if you pack up and leave early. 
* MP2 posted.


## Todays Topics/Activities  

### 1. Chalk Talk

* Start MP2 today.
* We'll go solutions to Lec 8 regression exercises and start logistic regression during the first half of Monday's lecture.



***



# Lec 8: Mon 2/25

## Announcements

* MP1 grades on Wednesday
* MP2 assigned on Wednesday. It will involve regression modeling on house price data again.
* For those of you with an interest in biostatistics, check out Dr. Emma Benn's talk on Tuesday March 5, 5pm in Seelye Hall 106.  
![](static/events/emma_benn.png){ width=500px }


## Todays Topics/Activities  

### 1. Chalk Talk

* Regression
* See [code](code.html#regression)

### 2. Tweet of the day

Dr. Benn is excited for her talk at Smith College SDS. **Are you?**

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Yessss!!! Definitely looking forward to speaking with and learning from the amazing <a href="https://twitter.com/SmithCollegeSDS?ref_src=twsrc%5Etfw">@SmithCollegeSDS</a> faculty and students! ü§óüíÉüèæü§óüíÉüèæ <a href="https://t.co/te01giEvXv">https://t.co/te01giEvXv</a></p>&mdash; Emma Benn (@EKTBenn) <a href="https://twitter.com/EKTBenn/status/1098396480719572993?ref_src=twsrc%5Etfw">February 21, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



***



# Lec 7: Mon 2/18

## Announcements

* Wednesday 2/20 is Rally Day: no lecture nor office hours. 
* Extra office hours Friday 2/22 3:00pm-4:15pm.
* Midterm I discussion

## Todays Topics/Activities  

### 1. Chalk Talk

* MP1 discussion




***



# Lec 6: Wed 2/13

## Announcements

Before Lec07 on Monday 2/18:  

* I will post ~~solutions~~ one of many possible approaches to doing `MP1.Rmd` in the [Mini-Projects](MP.html) page by Friday afternoon. Please read these.
* Delete the minimally viable `regression.Rmd` file I sent on Slack during Lec05 on Monday, replace it with this improved file <a href="static/methods/regression/regression_v2.Rmd" download>`regression_v2.Rmd`</a>, and please read it.

Also, I fixed the error in Lec03 exercises solutions in [Code](code.html) page. The issue was inconsistent converting from &deg;F to &deg;C in all `training` and `test` sets; I commented all such code out. The correct visualization of a spline model "trained on June and predicting December temperatures" is below. Observe that the predictions (in blue) overpredict the actual December temperatures (black dots).

![](static/images/train_june_test_december.png){ width=600px }

Remember:

> For your predictions to be valid, the data used to train your model must be at least somewhat representative of the test data you're making predictions on!

In other words:

If you only train your self-driving car here:  | Don't expect it to drive well here:
:-------------------------:|:-------------------------:
![](static/images/driving_in_circles.jpg){ height=1.71.7in }  |  ![](static/images/BOS_streets.png){ height=1.7}



## Todays Topics/Activities  

### 1. Chalk Talk

Brief discussion on "looking at your data" inspired by tweet of the day below.

### 2. Tweet of the day

Why is important to `glimpse()` and `View()` your data frames? Because **looking at your data** is so deceptively simple that many people forget or ignore this step, even analysts/engineers with PhD's at Google! Before any modeling, you must getting a sense of:

1. **What types of variables you have in your columns?** Numerical, categorical, text, dates?
1. **What values you have in your cells?** Units of any measurements?
1. **What is the quality of your data?** Do you have missing data? Are there crazy outliers?

These are the most fundamental steps to take before any modeling or data analysis!  

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Shout-out to people like me who like taking a look at even huge tables of raw data before summarizing or transforming, to get a sense of what&#39;s in there and spot potential data quality issues. Step 1 in data exploration! <a href="https://t.co/wVLHWcVNWA">pic.twitter.com/wVLHWcVNWA</a></p>&mdash; Data Science Renee (@BecomingDataSci) <a href="https://twitter.com/BecomingDataSci/status/1095421569428529153?ref_src=twsrc%5Etfw">February 12, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



***



# Lec 5: Mon 2/11

## Announcements

* Slack `#general` announcement about new `#mp1` channel. It is your responsibility to stay on top of all messages sent in this channel.
* Added clarification on what external sources you can use in [MP1 project instructions](MP.html#MP1-working).
* Wednesday's lecture is devoted to working on MP1


## Todays Topics/Activities  

### 1. Chalk Talk

* Discussion of solutions to Lec03 code exercises.
* Recap of Lec04: crossvalidation
* Regression

### 2. Tweet of the day

Shared with me by Dr. Jenny Smetzer. I agree with Ms. Damour's opinion.  You don't have to, but her ideas are still worth thinking about. They relate to "points of diminishing returns" and "polishing the cannonball" as discussed for mini-project 1. Here are some excerpts:

* "Overqualified and overprepared, too many women still hold back. Women feel confident only when they are perfect." IMO this applies to more demographic groups than just women.
* "First, parents and teachers can stop praising inefficient overwork, even if it results in good grades."
* "A colleague of mine likes to remind teenagers that in classes where any score above 90 counts as an A, the different between a 91 and a 99 is a life." In other words, [work-life balance](syllabus.html#work-life-balance). 
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Girls need to become tactical about school, to figure out how to get the same good grades, while doing a little bit less <a href="https://t.co/Achouetew6">https://t.co/Achouetew6</a></p>&mdash; NYT Opinion (@nytopinion) <a href="https://twitter.com/nytopinion/status/1093605420244049920?ref_src=twsrc%5Etfw">February 7, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



***



# Lec 4: Wed 2/6

## Announcements

* [DataFest](http://www.science.smith.edu/datafest/){target="_blank"} is a weekend-long "data science hackathon" for teams of up to 5 students and is at UMass the weekend of March 29-31! [Info session](static/other/datafest2019.pdf){target="_blank"} by Prof. Randi Garcia today 7-8pm in Sabin-Reed 301!
* "Multiple Imputation Methods in Cluster Randomzied Trials" talk by Prof. Brittney Bailey from Amherst College is tomorrow 12:10pm in McConnell B15. Free `r emo::ji("pizza")`!!!

<center>
![](static/other/bailey.png){ width=600px }
</center>


## Todays Topics/Activities  

### 1. Chalk Talk

* Mini-project 1 is now assigned; see [Mini-Projects](MP.html) page.
* Recap of Lec03 + exercise solutions.
* Crossvalidation. See images:
    + **Image 1: Validation set approach**:  
    ![](static/images/validation_set.png){ width=500px }
    + **Image 2: k = 5 fold crossvalidation**:  
    ![](static/images/k-fold-CV.png){ width=500px }
    + **Image 3: Leave-one-out crossvalidation (LOOCV)**:  
    ![](static/images/LOOCV.png){ width=500px }



***



# Lec 3: Mon 2/4

## Announcements

* Slack: Go over `#questions`.
* Syllabus finalized
* Explanation of readings in schedule above.
* Mini-project 1 groups posted; see [Mini-Projects](MP.html) page. MP1 on splines itself will be assigned on Wednesday after we cover crossvalidation.


## Todays Topics/Activities  

### 1. Chalk Talk

* Recap of Lec02.
* Splines are piecewise continuous cubic polynomials with [smoothness](https://en.wikipedia.org/wiki/Smoothness) constraints (1^st^ and 2^nd^ derivatives are 0 at all knots) See bottom-left plot below.
* Optional reading only if you're curious: How are splines fit? Linear algebra! See page 93-94 of this [PDF](https://people.cs.clemson.edu/~dhouse/courses/405/notes/splines.pdf){target="_blank"}.

![](static/images/splines.png){ width=700px }


### 2. In-class exercise

Go over code for Lec03 and do the exercises.


### 3. Screencast on splines

The code this screencast was based on can be found [here](http://bit.ly/rudeboybert_splines){target="_blank"}.

<iframe width="560" height="315" src="https://www.youtube.com/embed/bESJ81dyYro" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



***


# Lec 2: Wed 1/30

## Announcements

* Slack: Vote in two latest `#polls` and go over `#questions`.
* Go over updated schedule above, in particular "Important Dates"
* DataCamp "Modeling with Data in the tidyverse" assigned:
    + Make sure you can see the class DataCamp group by clicking [here](https://www.datacamp.com/enterprise/2019-01-machine-learning-293){target="_blank"}. If you cannot, then read the Google Doc in Lec01
    + Due Wed 2/6 at 1pm. Should take about 4 hours.
    + Only thing that is graded is whether you complete it, not the number of points. 
    + This assumes familiarity with the `tidyverse` suite of R packages; see the December email in the Google Doc in Lec01.
    + The DataCamp course builds up to Chapter 4's "Validation set prediction framework"



## Todays Topics/Activities  

### 1. Chalk Talk

* Recap of Lec01: $f(\vec{x})$ and $\hat{f}(\vec{x})$
* Based on code for `Lec02` posted in [Code](code.html) tab of menu bar for course webpage.
* Check out [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques){target="_blank"} Kaggle competition.
* **Added after lecture**: Link to ModernDive chapter with an example on [logarithmic transformations](https://moderndive.netlify.com/12-thinking-with-data.html#log10-transformations){target="_blank"}. You'll also be seeing log-transformations in this week's DataCamp assignment!

### 2. Tweet of the day

This class is just part of a larger phenomenon of resources spread *very* thin:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">On campuses across America, the surge in student demand for computer science courses is far outstripping the supply of professors <a href="https://t.co/9dwUMZ0eg8">https://t.co/9dwUMZ0eg8</a></p>&mdash; The New York Times (@nytimes) <a href="https://twitter.com/nytimes/status/1088440053796356107?ref_src=twsrc%5Etfw">January 24, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>





***






# Lec 1: Mon 1/27

## Announcements

* If you haven‚Äôt already, please go over the following [Google Doc](https://docs.google.com/document/d/1NfWkoyeiovkVyMmgmKFGxCMusyxBMYHVvsAUjEZv_fU/){target="_blank"} with important information about the course and steps you should complete to get setup, in particular the Intro Survey. The whole process should take about 20 minutes.
* **Added on Sun 1/27 6PM**: Create an account on Kaggle.com using your Smith (or Five College) email address. 



## Todays Topics/Activities       

### 1. Chalk Talk

*Why chalk talks? Read the [Field Notes](https://twitter.com/FieldNotesBrand){target="_blank"} slogan.*

[Lec01 slides](http://rudeboybert.rbind.io/talk/2019-01-13-Williams.pdf){target="_blank"} + 2 chalk talks. 


### 2. Background I will assume

1. You have seen multiple regression before (not necessarily taken a class on it).
1. The `tidyverse` suite of R packages for data science, in particular `ggplot2` and `dplyr`.

For more information about what level you should know this, please see the [Google Doc](https://docs.google.com/document/d/1NfWkoyeiovkVyMmgmKFGxCMusyxBMYHVvsAUjEZv_fU/){target="_blank"} -> Email from December.


### 3. Prepartion for Lec02

I **highly** recommend you start these steps as soon as you can, so that you'll have two chances to go to the Spinelli tutoring hours if you need help before Wednesday's lecture. Note the Spinelli tutoring hours are Sun-Thurs 7-9pm in Sabin-Reed 301.

1. Have R and RStudio installed on your laptops. 
1. Install some extra tools:
    + macOS users: Go to Apple's [developer page](https://developer.apple.com/download/more/){target="_blank"} and install the appropriate version of Command Line Tools. Hint: Google "How do I know what version of macOS I have?"
    + Windows users: Install [`Rtools35.exe`](https://cran.r-project.org/bin/windows/Rtools/){target="_blank"}.
1. Install the standard versions of the `tidyverse` and `devtools` packages as you normally would.
1. Install the *development version* of the `moderndive` package by running: `devtools::install_github("moderndive/moderndive")`
1. Run the code below. If you get no errors and a file `submission.csv` gets saved to your computer, you're ready for Lec02!

```{r}
library(tidyverse)
library(moderndive)

# 1. Load in training and test data
train <- read_csv("https://rudeboybert.github.io/SDS293/static/train.csv")
test <- read_csv("https://rudeboybert.github.io/SDS293/static/test.csv")

# 2. Fit model on training data
house_model <- lm(SalePrice ~ YrSold, data = train)

# 3. Apply fitted model to get predictions for test data
submission <- get_regression_points(house_model, newdata = test, ID = "Id") %>% 
  select(Id, SalePrice = SalePrice_hat)

# 4. Output predictions to CSV
write_csv(submission, "submission.csv")
```

