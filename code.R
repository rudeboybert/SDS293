#------------------------------------------------------------------------------
# Lec14: 2019/03/25
#------------------------------------------------------------------------------
library(tidyverse)
# Pre-process iris dataset
iris <- iris %>%
  # Convert to tibble data frame:
  as_tibble() %>%
  # Add identification variable to uniquely identify each row:
  rownames_to_column(var="ID")


# Fit CART model, in this case for classification
library(rpart)
model_formula <- as.formula(Species ~ Sepal.Length + Sepal.Width)
tree_parameters <- rpart.control(maxdepth = 3)
model_CART <- rpart(model_formula, data = iris, control = tree_parameters)

# Plot CART model
plot(model_CART, margin=0.25)
text(model_CART, use.n = TRUE)
title("Predicting iris species using sepal length & width")
box()


#------------------------------------------------------------------------------
# Exercises with your partner:

# a) If the condition at a given node of the tree evaluates to true, do you go
# down the left branch or the right branch?
# b) Note the bottom-left most "leaf" 44/1/0, corresponds to 44 setosa, 1
# versicolor, 0 virginia, and thus the "majority" winner is setosa. Apply a
# sequence of dplyr commands to the iris data frame to end up with a data frame
# of 44 + 1 + 0 = 45 rows corresponding to these 45 flowers
# c) Read the help file for `rpart.control` and play around with different
# arguments that control the shape of the tree in the tree_parameters object
# above:
tree_parameters_2 <- rpart.control(CHANGE THIS)



# Create training (100 flowers) and test (50 flowers)
set.seed(76)
iris_train <- iris %>%
  sample_frac(2/3)
iris_test <- iris %>%
  anti_join(iris_train, by = "ID")

# 1.a) Fit model to train
model_CART_2 <- rpart(model_formula, data = iris_train, control = tree_parameters)

# 1.b) Plot CART model
plot(model_CART_2, margin = 0.25)
text(model_CART_2, use.n = TRUE)
title("Predicting iris species using sepal length & width")
box()

# 1.c) Get fitted probabilities for each class on train
p_hat_matrix_train <- model_CART_2 %>%
  predict(type = "prob", newdata = iris_train) %>%
  # Convert matrix object to data frame:
  as_tibble()
p_hat_matrix_train

# 1.d) Look at distinct probabilities
p_hat_matrix_train %>%
  distinct()

# 2.a) Apply model to test to get fitted probabilities for each class
p_hat_matrix_test <- model_CART_2 %>%
  predict(type = "prob", newdata = iris_test) %>%
  # Convert matrix object to data frame:
  as_tibble()
p_hat_matrix_test

# 2.b) Instead of fitted probabilities, return fitted y's, where highest
# probability wins and ties are broken at random
y_hat <- model_CART %>%
  predict(type="class", newdata = iris) %>%
  # Function to convert a vector to a data frame
  enframe()
y_hat



# Look at help file for the (multi-class) logarithmic loss function, which is
# one possible "score" for categorical variables when you have more than 2
# categories.
library(yardstick)
?mn_log_loss

# Create a new data frame:
bind_cols(
  # Observed y:
  Species = iris_test$Species,
  # Fitted probabilities for each class
  p_hat_matrix_test
) %>%
  # Compute multi-class log-loss
  mn_log_loss(truth = Species, c(setosa, versicolor, virginica))


#------------------------------------------------------------------------------
# Exercises with your partner:

# d) In 1.d) you saw there are only 3 unique possible 3-tuples (i.e. triplets)
# of fitted probabilties. Which leaf in the tree does each of these 4 possible
# 3-tuples correspond to?

# e) Are larger (multi-class) logarithmic loss function indicative of better
# predictions or worse predictions?





#------------------------------------------------------------------------------
# Solutions

# a) Looking at the top node of the plot of model_CART and going left, there are
# total of 44 + 1 + 0 + 1 + 5 + 1 = 52 flowers in all children leaves. Since
iris %>%
  filter(Sepal.Length < 5.45) %>%
  nrow()
# yields a data frame with 52 rows, if the boolean evaluates to true, then you
# go left

# b) Note there are 0 virginica:
iris %>%
  filter(Sepal.Length < 5.45) %>%
  filter(Sepal.Width >= 2.8) %>%
  count(Species)

# c) Let's set the minsplit to 50 for example
tree_parameters_2 <- rpart.control(minsplit = 100)
model_CART_3 <- rpart(model_formula, data = iris, control = tree_parameters_2)

# Plot CART model. Once there are less than 100 trees at a node, we stop
# splitting
plot(model_CART_3, margin=0.25)
text(model_CART_3, use.n = TRUE)
title("Predicting iris species using sepal length & width")
box()

# d)
p_hat_matrix_train %>%
  distinct()
# First row above is the 32/4/0 leaf, since we have probabilities of
# 32/36 = 0.889, 4/36 = 0.111, 0/36. The winner is setosa
# Second row above is the 1/19/30 leaf, thus the winner is virginica
# Third row above is the 3/11/0 row, thus the winnder is versicolor

# e) Look at: https://cdn-images-1.medium.com/max/1600/0*i2_eUc_t8A1EJObd.png if
# p_ij = 1, the log(p_ij) = 0, and thus sum = 0, and thus the whole thing is 0
# Thus low (multi-class) logarithmic loss function are indicative of good
# predictions


# Figure from Lec
region_labels <- tibble(
  Sepal.Length = c(4.5, 4.5, 7, 5.95, 5.7),
  Sepal.Width = c(4.4, 2.1, 4.4, 4.4, 2.1),
  label = c("R1", "R2", "R5", "R3", "R4")
)

ggplot(iris, aes(x=Sepal.Length, y = Sepal.Width)) +
  geom_jitter(aes(col = Species)) +
  annotate("segment", x = 5.45, xend = 5.45, y = 2, yend = 4.5, size = 1) +
  annotate("segment", x = 4, xend = 5.45, y = 2.8, yend = 2.8, size = 1) +
  annotate("segment", x = 6.15, xend = 6.15, y = 2, yend = 4.5, size = 1) +
  annotate("segment", x = 6.15, xend = 5.45, y = 3.1, yend = 3.1, size = 1) +
  geom_text(data = region_labels, aes(label = label), size = 10) +
  labs(x = "x1: Sepal Length", y = "x2: Sepal Width",
       title = "Jittered scatterplot of CART", col = "y: Species")

ggsave("static/methods/CART/scatterplot.png", width = 16/1.8, height = 9/1.8)







#------------------------------------------------------------------------------
# Lec09: 2019/03/04
#------------------------------------------------------------------------------
library(tidyverse)
library(broom)

# Read in training data from https://www.kaggle.com/c/GiveMeSomeCredit/
financial_distress_orig <-
  "https://rudeboybert.github.io/SDS293/static/methods/logisitic/cs-training.csv" %>%
  read_csv() %>%
  select(ID = X1, in_financial_distress = SeriousDlqin2yrs, age)

# Let's deliberately tinker and engineer this data for educational purposes
# only: For those individuals who are in financial distress, let's add an offset
# of 50 to their ages
offset <- 50
financial_distress <- financial_distress_orig %>%
  mutate(age = ifelse(in_financial_distress == 1, age + offset, age))

# Split data into train and test so that we can fit to train and predict on
# test. Note that this corresponds to the "validation set" approach that is
# used mostly for illustrative purposes and not used in practice as using this
# approach you wouldn't be making predictions on every observation.
# Be sure to View() these data frames after you create them:
set.seed(76)
cs_training <- financial_distress %>%
  sample_frac(0.25)
cs_test <- financial_distress %>%
  anti_join(cs_training, by="ID")

# EDA: Recall that we engineering the two boxplots to not overlap by adding an
# offset
ggplot(cs_training, aes(x = as.logical(in_financial_distress), y = age)) +
  geom_boxplot() +
  labs(x = "In financial distress?", y = "Age")

# Let's create a scatterplot but with age on the x-axis. Note this plot suffers
# from overplotting:
ggplot(cs_training, aes(x = age, y = in_financial_distress)) +
  geom_point() +
  labs(x = "Age", y = "In financial distress?")

# Let's "jitter" the plot a little to break up the overplotting. In other words,
# add random vertical "nudges" to the points so that we can get a sense of how
# many plots are on top of each other. Note this is only a visualization tool;
# it does not alter the original values in the data frame.
# For more info on geom_jitter read:
# https://moderndive.netlify.com/3-viz.html#overplotting
ggplot(cs_training, aes(x = age, y = in_financial_distress)) +
  geom_jitter(height = 0.01) +
  labs(x = "Age", y = "In financial distress?")

# The best fitting linear regression line in blue is no good in this particular
# case; you end up with fitted probabilities less than 0
ggplot(cs_training, aes(x = age, y = in_financial_distress)) +
  geom_jitter(height = 0.01) +
  labs(x = "Age", y = "In financial distress?") +
  geom_smooth(method = "lm", se = FALSE)

# Fit a logistic regression model. Note the use of glm() instead of lm()
model_logistic <- glm(in_financial_distress ~ age, family = "binomial", data = cs_training)

# 2.a) Extract regression table with confidence intervals
# Notice coefficient for age. Is it positive or negative?
model_logistic %>%
  broom::tidy(conf.int = TRUE)

# 2.b) Extract point-by-point info of points used to fit model
fitted_points_logistic <- model_logistic %>%
  broom::augment()
fitted_points_logistic

# The .fitted values are the fitted log-odds however, NOT fitted probabilities.
# We convert to fitted probabilities using inverse-logit function:
fitted_points_logistic <- fitted_points_logistic %>%
  mutate(fitted_prob = 1/(1 + exp(-.fitted)))
fitted_points_logistic

# 2.c) Extract model summary info
model_logistic %>%
  broom::glance()

# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points_logistic
predicted_points_logistic <- model_logistic %>%
  broom::augment(newdata = cs_test)
predicted_points_logistic

# 4. Visualize fitted model only the training data for now:
ggplot(data = fitted_points_logistic, aes(x = age, y = in_financial_distress)) +
  # Training data with black points:
  geom_jitter(height = 0.01) +
  # Best fitting linear regression line in blue:
  geom_smooth(method = "lm", se = FALSE) +
  # Best fitting logistic curve in red:
  geom_line(data = fitted_points_logistic, mapping = aes(y = fitted_prob), col = "red", size = 1) +
  labs(x = "Age", y = "In financial distress?")


#------------------------------------------------------------------------------
# Exercises

# 1. Using the visualization above, for what age would you say that there is a
# 50% probability that an individual is in financial distress?
# Maybe around 87?

# 2. Compare the visualization above with a scatterplot with:
# a) x = age
# b) y = the observed proportion of individuals in cs_training that are in
# financial
observed_proportions <- cs_training %>%
  group_by(age) %>%
  summarize(prop = mean(in_financial_distress))

ggplot(data = fitted_points_logistic, aes(x = age, y = in_financial_distress)) +
  # Training data with black points:
  geom_jitter(height = 0.01) +
  # Best fitting linear regression line in blue:
  geom_smooth(method = "lm", se = FALSE) +
  # Best fitting logistic curve in red:
  geom_line(data = fitted_points_logistic, mapping = aes(y = fitted_prob), col = "red", size = 1) +
  labs(x = "Age", y = "In financial distress?") +
  geom_line(data = observed_proportions, aes(x = age, y = prop), col = "orange", size = 1)

# 3. Change the offset in age to 10 and -50. What do you notice happens to:
# a) the coefficient for age in the regression table.
# b) the shape of the logistic curve of the fitted model?
#
# Offset 50:
# a) Coefficient = 0.239
# b) Shape: very S-like,

# Offset 10:
# a) Coefficient = 0.0132
# b) Shape: less S-like

# Offset -50:
# a) Coefficient = -0.467
# b) Shape: Very inverse S-like. (looks like a "Z" or "2" instead of "S")

# 4. Challenge question: Change the offset in age to 6.9. Why is the logistic curve
# flat? At what value is it?
financial_distress %>%
  group_by(in_financial_distress) %>%
  summarize(avg_age = mean(age))
# Both groups have the same mean age, so there is no information provided by
# the variable age. The red line is at the total proportion of people in
# financial distress irrespective of group = 0.0668 = 6.68%
financial_distress %>%
  summarize(overall_prop = mean(in_financial_distress))




#------------------------------------------------------------------------------
# Lec08: 2019/02/25
#------------------------------------------------------------------------------
library(tidyverse)
library(broom)
library(stringr)


#------------------------------------------------------------------------------
# Data for today
# Read over the help file
?mtcars

# Data wrangling
mtcars <- mtcars %>%
  # Convert to tibble data frame:
  as_tibble() %>%
  # Add identification variable as first column:
  mutate(ID = 1:n()) %>%
  select(ID, everything()) %>%
  # vs & am variables were recorded as numerical 0/1, but are really categorical
  # so convert them
  mutate(
    vs = ifelse(vs == 0, "V-shaped", "straight"),
    am = ifelse(am == 0, "automatic", "manual")
  )

# Set up validation set framework: create training and test set at random
set.seed(76)
mtcars_train <- mtcars %>%
  sample_frac(0.75)
mtcars_test <- mtcars %>%
  anti_join(mtcars_train, by="ID")

glimpse(mtcars_train)
glimpse(mtcars_test)


#------------------------------------------------------------------------------
# Model 1:
# y: fuel efficiency: measured in miles per gallon
# x: engine horsepower: unit of power where 1hp = work needed to lift 75kg a
# vertical distance of 1 meter in 1 second: https://en.wikipedia.org/wiki/Horsepower

# 1. Fit model to training data
model_1_formula <- as.formula("mpg ~ hp")
model_1 <- lm(model_1_formula, data = mtcars_train)

# 2.a) Extract regression table with confidence intervals
model_1 %>%
  broom::tidy(conf.int = TRUE)

# 2.b) Extract point-by-point info of points used to fit model
fitted_points_1 <- model_1 %>%
  broom::augment()
fitted_points_1

# 2.c) Extract model summary info
model_1 %>%
  broom::glance()

# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_1 <- model_1 %>%
  broom::augment(newdata = mtcars_test)
predicted_points_1

# 4. Visualize
ggplot(NULL) +
  # Training data with black points:
  geom_point(data = fitted_points_1, aes(x = hp, y = mpg)) +
  # Fitted simple linear regression model with blue line:
  geom_line(data = fitted_points_1, aes(x = hp, y = .fitted), col = "blue") +
  # Predictions for test set with red points
  geom_point(data = predicted_points_1, aes(x = hp, y = .fitted), col = "red") +
  labs(x = "Horse power", y = "Miles per gallon")


#------------------------------------------------------------------------------
# Model 2:
# y: fuel efficiency: measured in miles per gallon
# x: All numerical predictors

# 1. Fit model to training data
model_2_formula <- as.formula("mpg ~ cyl + disp + hp + drat + wt + qsec + gear + carb")
model_2 <- lm(model_2_formula, data = mtcars_train)

# 1. Here is a hint for generating the right-hand side (RHS) of formulas when
# you have a lot of predictors. Note how I removed ID and mpg from RHS.
names(mtcars) %>%
  str_c(collapse = " + ")

# 2.a) Extract regression table with confidence intervals
model_2 %>%
  broom::tidy(conf.int = TRUE)

# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
  broom::augment()
fitted_points_2

# 2.c) Extract model summary info
model_2 %>%
  broom::glance()

# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
  broom::augment(newdata = mtcars_test)
predicted_points_2


#------------------------------------------------------------------------------
# Lec08 Exercises. Solutions at bottom

# 1. What are the test set RMSEs of Models 1 & 2? Which is higher?

# 2. What is the ratio of n/p for our trained Model 2. i.e. the number of points
# in the training set vs the number of predictors

# 3. Change the train/test validation ratio from 3:1 to 1:1. What are the RMSEs
# of Models 1 & 2? Which is higher?

# 4. What is the new ratio of n/p for our new trained Model 2?

# 5. How does the difference in test set RMSE for Model 1 & 2 itself differ when the
# train/test validation ratio went from 3:1 to 1:1

# 6. Try a different combination of variables and see if you can lower your
# RMSE.


#------------------------------------------------------------------------------
# Lec10: 2019/03/04
#------------------------------------------------------------------------------
library(tidyverse)
library(broom)

# Read in training data from https://www.kaggle.com/c/GiveMeSomeCredit/
financial_distress_orig <-
  "https://rudeboybert.github.io/SDS293/static/methods/logisitic/cs-training.csv" %>%
  read_csv() %>%
  select(ID = X1, in_financial_distress = SeriousDlqin2yrs, age)

# Let's deliberately tinker and engineer this data for educational purposes
# only: For those individuals who are in financial distress, let's add an offset
# of 50 to their ages
offset <- 50
financial_distress <- financial_distress_orig %>%
  mutate(age = ifelse(in_financial_distress == 1, age + offset, age))

# Split data into train and test so that we can fit to train and predict on
# test. Note that this corresponds to the "validation set" approach that is
# used mostly for illustrative purposes and not used in practice as using this
# approach you wouldn't be making predictions on every observation.
# Be sure to View() these data frames after you create them:
set.seed(76)
cs_training <- financial_distress %>%
  sample_frac(0.25)
cs_test <- financial_distress %>%
  anti_join(cs_training, by="ID")

# EDA: Recall that we engineering the two boxplots to not overlap by adding an
# offset
ggplot(cs_training, aes(x = as.logical(in_financial_distress), y = age)) +
  geom_boxplot() +
  labs(x = "In financial distress?", y = "Age")

# Let's create a scatterplot but with age on the x-axis. Note this plot suffers
# from overplotting:
ggplot(cs_training, aes(x = age, y = in_financial_distress)) +
  geom_point() +
  labs(x = "Age", y = "In financial distress?")

# Let's "jitter" the plot a little to break up the overplotting. In other words,
# add random vertical "nudges" to the points so that we can get a sense of how
# many plots are on top of each other. Note this is only a visualization tool;
# it does not alter the original values in the data frame.
# For more info on geom_jitter read:
# https://moderndive.netlify.com/3-viz.html#overplotting
ggplot(cs_training, aes(x = age, y = in_financial_distress)) +
  geom_jitter(height = 0.01) +
  labs(x = "Age", y = "In financial distress?")

# The best fitting linear regression line in blue is no good in this particular
# case; you end up with fitted probabilities less than 0
ggplot(cs_training, aes(x = age, y = in_financial_distress)) +
  geom_jitter(height = 0.01) +
  labs(x = "Age", y = "In financial distress?") +
  geom_smooth(method = "lm", se = FALSE)

# Fit a logistic regression model. Note the use of glm() instead of lm()
model_logistic <- glm(in_financial_distress ~ age, family = "binomial", data = cs_training)

# 2.a) Extract regression table with confidence intervals
# Notice coefficient for age. Is it positive or negative?
model_logistic %>%
  broom::tidy(conf.int = TRUE)

# 2.b) Extract point-by-point info of points used to fit model
fitted_points_logistic <- model_logistic %>%
  broom::augment()
fitted_points_logistic

# The .fitted values are the fitted log-odds however, NOT fitted probabilities.
# We convert to fitted probabilities using inverse-logit function:
fitted_points_logistic <- fitted_points_logistic %>%
  mutate(fitted_prob = 1/(1 + exp(-.fitted)))
fitted_points_logistic

# 2.c) Extract model summary info
model_logistic %>%
  broom::glance()

# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points_logistic
predicted_points_logistic <- model_logistic %>%
  broom::augment(newdata = cs_test)
predicted_points_logistic

# 4. Visualize fitted model only the training data for now:
ggplot(data = fitted_points_logistic, aes(x = age, y = in_financial_distress)) +
  # Training data with black points:
  geom_jitter(height = 0.01) +
  # Best fitting linear regression line in blue:
  geom_smooth(method = "lm", se = FALSE) +
  # Best fitting logistic curve in red:
  geom_line(data = fitted_points_logistic, mapping = aes(y = fitted_prob), col = "red", size = 1) +
  labs(x = "Age", y = "In financial distress?")


#------------------------------------------------------------------------------
# Lec10 Exercises. Solutions at bottom
# 1. Using the visualization above, for what age would you say that there is a

# 2. Compare the visualization above with a scatterplot with:
# a) x = age
# b) y = the observed proportion of individuals in cs_training that are in
# financial

# 3. Change the offset in age to 10 and -50. What do you notice happens to:
# a) the coefficient for age in the regression table.
# b) the shape of the logistic curve of the fitted model?

# 4. Challenge question: Change the offset in age to 6.9. Why is the logistic curve
# flat? At what value is it?


#------------------------------------------------------------------------------
# Lec08 Solutions

# 1. What are the test set RMSEs of Models 1 & 2? Which is higher?
# Being sure to set.seed(76)
predicted_points_1 %>%
  yardstick::rmse(truth = mpg, estimate = .fitted)
predicted_points_2 %>%
  yardstick::rmse(truth = mpg, estimate = .fitted)
# Model 1 = 5.38 < 9.05 = Model 2

# 2. What is the ratio of n/p for our trained Model 2. i.e. the number of points
# in the training set vs the number of predictors
# n = 24, p = 8, thus n/p = 3

# 3. Change the train/test validation ratio from 3:1 to 1:1. What are the RMSEs
# of Models 1 & 2? Which is higher?
# Being sure to set.seed(76)
# Model 1 = 5.05 < 7.89 = Model 2

# 4. What is the new ratio of n/p for our new trained Model 2?
# n = 16, p = 8, thus n/p = 2

# 5. How does the difference in test set RMSE for Model 1 & 2 itself differ when the
# train/test validation ratio went from 3:1 to 1:1
# 5.38 vs 9.05
# 5.05 vs 7.89

# Overfitting is less of a problem when the ratio n/p is smaller

# 6. Try a different combination of variables and see if you can lower your
# RMSE.



#------------------------------------------------------------------------------
# Lec10 Solutions

# 1. Using the visualization above, for what age would you say that there is a
# 50% probability that an individual is in financial distress?
# Maybe around 87?

# 2. Compare the visualization above with a scatterplot with:
# a) x = age
# b) y = the observed proportion of individuals in cs_training that are in
# financial
observed_proportions <- cs_training %>%
  group_by(age) %>%
  summarize(prop = mean(in_financial_distress))

ggplot(data = fitted_points_logistic, aes(x = age, y = in_financial_distress)) +
  # Training data with black points:
  geom_jitter(height = 0.01) +
  # Best fitting linear regression line in blue:
  geom_smooth(method = "lm", se = FALSE) +
  # Best fitting logistic curve in red:
  geom_line(data = fitted_points_logistic, mapping = aes(y = fitted_prob), col = "red", size = 1) +
  labs(x = "Age", y = "In financial distress?") +
  geom_line(data = observed_proportions, aes(x = age, y = prop), col = "orange", size = 1)

# 3. Change the offset in age to 10 and -50. What do you notice happens to:
# a) the coefficient for age in the regression table.
# b) the shape of the logistic curve of the fitted model?
#
# Offset 50:
# a) Coefficient = 0.239
# b) Shape: very S-like,

# Offset 10:
# a) Coefficient = 0.0132
# b) Shape: less S-like

# Offset -50:
# a) Coefficient = -0.467
# b) Shape: Very inverse S-like. (looks like a "Z" or "2" instead of "S")

# 4. Challenge question: Change the offset in age to 6.9. Why is the logistic curve
# flat? At what value is it?
financial_distress %>%
  group_by(in_financial_distress) %>%
  summarize(avg_age = mean(age))
# Both groups have the same mean age, so there is no information provided by
# the variable age. The red line is at the total proportion of people in
# financial distress irrespective of group = 0.0668 = 6.68%
financial_distress %>%
  summarize(overall_prop = mean(in_financial_distress))






#------------------------------------------------------------------------------
# Lec02: 2019/01/30
#------------------------------------------------------------------------------
library(tidyverse)
library(moderndive)

# 1. Load in training and test data
train <- read_csv("https://rudeboybert.github.io/SDS293/static/train.csv")
test <- read_csv("https://rudeboybert.github.io/SDS293/static/test.csv")

# 2. Fit model on training data
house_model <- lm(SalePrice ~ YrSold, data = train)

# 3. Apply fitted model to get predictions for test data
submission <- get_regression_points(house_model, newdata = test, ID = "Id") %>%
  select(Id, SalePrice = SalePrice_hat)

# 4. Output predictions to CSV
write_csv(submission, "submission.csv")



#------------------------------------------------------------------------------
# Lec03: 2019/02/04 Splines
#------------------------------------------------------------------------------
library(tidyverse)
library(nycflights13)
library(broom)

#------------------------------------------------------------------------------
# Create training data and perform exploratory data analysis
# Define training data: hourly temperature recordings at JFK airport in June 2013
training <- weather %>%
  filter(origin == "JFK", month == 6) %>%
  select(time_hour, temp, humid)
training

# Optional: convert temperature from F to C
# training <- training %>%
#   mutate(temp = (temp-32)/1.8)

# Always do an exploratory data analysis first!

# Histogram of outcome variable: temperature
ggplot(training, aes(x = temp)) +
  geom_histogram()

# Histogram of relationship of outcome variable and predictor variable: humidity
training_plot <- ggplot(training, aes(x = humid, y = temp)) +
  geom_point()
training_plot


#------------------------------------------------------------------------------
# Step 1: Fit spline model to training data and save in model_spline object.
fitted_spline_model <- smooth.spline(x = training$humid, y = training$temp, df = 10)

# Extract data frame of info based on fitted model:
fitted_spline_model_points <- fitted_spline_model %>%
  broom::augment()
fitted_spline_model_points

# Plot fitted model on training data:
training_plot +
  geom_line(data = fitted_spline_model_points, aes(x = x, y = .fitted), col = "blue", size = 1)


#------------------------------------------------------------------------------
# Create test data: hourly temperature recordings at JFK airport in May 2013.
# Note here we "know" the outcome variable temp, but in a real Kaggle
# competition you won't!
test <-  weather %>%
  filter(origin == "JFK", month == 5) %>%
  select(time_hour, temp, humid)
test

# Optional: convert temperature from F to C
# test <- test %>%
#   mutate(temp = (temp-32)/1.8)


#------------------------------------------------------------------------------
# Step 2: Make predictions on test data by applying fitted_spline_model
predicted_points <- predict(fitted_spline_model, x = test$humid) %>%
  as_tibble()
predicted_points

# Plot!
ggplot() +
  geom_point(data = test, aes(x = humid, y = temp)) +
  geom_line(data = predicted_points, aes(x = x, y = y), col = "blue", size = 1)


#------------------------------------------------------------------------------
# Exercises: Not to be submitted. I will go over solutions next lecture
# 1. In Step 1, vary the complexity of the model by using values different
# values of df. What happens to you fitted model?
# 2. Change the test set to be measurements at JFK for December. How does our
# fitted model do?
# 3. What is the RMSE of your fitted model on June temperatures?
# July temperatures? December temperatures.

#------------------------------------------------------------------------------
# Solutions:
# 2. Recall in example above training was June data, test was May data. Redefine
# test data to be December
test <-  weather %>%
  filter(origin == "JFK", month == 12) %>%
  select(time_hour, temp, humid)

# Optional: convert temperature from F to C
# test <- test %>%
#   mutate(temp = (temp-32)/1.8)

# Get predicted values:
predicted_points <- predict(fitted_spline_model, x = test$humid) %>%
  as_tibble()

# Plot!
ggplot() +
  geom_point(data = test, aes(x = humid, y = temp)) +
  geom_line(data = predicted_points, aes(x = x, y = y), col = "blue", size = 1) +
  labs(title = "Trained on June, Tested on December")

# Add column of predicted values temp_hat to test
test <- test %>%
  mutate(temp_hat = predicted_points$y)
test

# 3. Many ways to compute RMSE for December
# From scratch
test %>%
  mutate(
    residual = temp - temp_hat,
    squared_residual = residual^2
  ) %>%
  summarize(mse = mean(squared_residual)) %>%
  mutate(rmse = sqrt(mse))

# Using existing functions:
library(yardstick)
test %>%
  yardstick::rmse(truth = temp, estimate = temp_hat)

library(MLmetrics)
MLmetrics::RMSE(y_pred = test$temp_hat, y_true = test$temp)

# 3. To compute RMSE for June, let's combine all the above steps into a
# single dplyr chain:
weather %>%
  # Set test set to be June data
  filter(origin == "JFK", month == 6) %>%
  select(time_hour, temp, humid) %>%
  # Get predicted values temp_hat:
  mutate(
    temp_hat = predict(fitted_spline_model, x = humid) %>% as_tibble() %>% pull(y)
  ) %>%
  # Compute RMSE:
  mutate(
    residual = temp - temp_hat,
    squared_residual = residual^2
  ) %>%
  summarize(mse = mean(squared_residual)) %>%
  mutate(rmse = sqrt(mse))
