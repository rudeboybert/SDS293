
---
title: "LASSO Regularization"
author: "Albert Y. Kim"
date: "Last updated on `r Sys.Date()`"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10
)

library(tidyverse)
library(broom)
library(stringr)
library(knitr)
library(moderndive)
library(glmnet)
library(ISLR)
library(plotly)

set.seed(76)
```


# Data

Let's consider data for $i=1, \ldots, 400$ individuals' credit card debt. Note this data was simulated and is not real. 

* $y_i$: Credit card balance i.e. credit card debt
* $x_{1,i}$: Income in $10K
* $x_{2,i}$: Credit limit in $

```{r}
credit <- Credit %>%
  select(Balance, Income, Limit)
```


Here is a random sample of 10 of the 400 rows:

```{r, echo=FALSE}
credit %>% 
  sample_n(10) %>% 
  kable()
```

Let's view the points in an interactive 3D plot:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
# Define base plotly plot
base_plot <-
  plot_ly(showlegend=FALSE) %>%
  add_markers(
    x = credit$Income,
    y = credit$Limit,
    z = credit$Balance,
    hoverinfo = 'text',
    text = ~paste("x1 - Income: ", credit$Income, "</br> x2 - Limit: ", 
                  credit$Limit, "</br> y - Balance: ", credit$Balance)
  ) %>%
  layout(
    scene = list(
      xaxis = list(title = "x1 - Income (in $10K)"),
      yaxis = list(title = "x2 - Limit ($)"),
      zaxis = list(title = "y - Balance ($)")
    )
  )

# Output base plot
# base_plot

# Define (x1, x2) grid of values (bottom plane). We'll use this values to 
# compute all regression planes
x_grid <- seq(from=min(credit$Income), to=max(credit$Income), length=100)
y_grid <- seq(from=min(credit$Limit), to=max(credit$Limit), length=200)
```



***



# Three Models

Let's now consider three models for credit card `Balance` i.e. credit card debt

1. **Naive Model**: Uses no predictor information.
1. **Simple linear regression model**: Uses only one predictor `Limit`.
1. **Multiple regression model**: Uses both predictors `Limit` and `Income`.



***



# 1. Naive Model

Say we use no predictor information. This corresponds to the following true model $f()$ and error component $\epsilon$.

$$
\begin{aligned}
y &= f(\vec{x}) + \epsilon\\
y &= \beta_0 + \epsilon\\
\mbox{Balance} &= \beta_0 + \epsilon
\end{aligned}
$$

In other words there is only an intercept term. Since the mean credit card balance AKA credit card debt $\bar{y}$ is:

```{r}
mean(credit$Balance)
```

We'll estimate/approximate $f()$ with the following fitted model $\widehat{f}()$:

$$
\begin{aligned}
\widehat{y} &= \widehat{f}(\vec{x})\\
\widehat{y} &= \widehat{\beta}_0\\
\widehat{\mbox{Balance}} &= \widehat{\beta}_0 \\
\widehat{\mbox{Balance}} &= \overline{y}
\end{aligned}
$$

In other words, think of the above fitted model $\widehat{f}(\vec{x})$ as a **minimally viable model**, in other words a "null" model, in other words a "basic baseline model". Using this model, our prediction $\widehat{y}$ of an individual's credit bard balance using no predictor information would be $\bar{y}$ = \$520.01. Let' visualize this in a histogram:

```{r, fig.width=8}
ggplot(credit, aes(x = Balance)) +
  geom_histogram(binwidth = 100, boundary = 0) +
  labs(x = "y = Balance ($)", title = "Histogram of outcome variable: credit card balance") +
  geom_vline(xintercept = mean(credit$Balance), col = "red", size = 1)
```


```{r, eval = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
# Compute z-axis values for this model. i.e. flat plane. We'll use this later:
z_grid_1 <- expand.grid(x_grid, y_grid) %>%
  tbl_df() %>%
  rename(
    x_grid = Var1,
    y_grid = Var2
  ) %>%
  mutate(z = mean(credit$Balance)) %>%
  .[["z"]] %>%
  matrix(nrow=length(x_grid)) %>%
  t()
```

Surely we can do better than this however! We are not using any of the information contained in the predictor variables $x_1$ `Income` and $x_2$ credit `Limit`. In other words, we are predicting $520.01 as the credit card debt irregardless of the individual's income and credit limit. 



***



# 2. Simple Linear Regression Model

Let's improve on our minimally viable model by using a predictor variable: $x_1$ `Income`. Let's assume the following is the truth: a true model $f()$ and error component $\epsilon$.

$$
\begin{aligned}
y &= f(\vec{x}) + \epsilon\\
y &= \beta_0 + \beta_1 x_1 + \epsilon\\
\mbox{Balance} &= \beta_0 + \beta_1\mbox{Income} + \epsilon
\end{aligned}
$$

We'll estimate/approximate $f()$ with the following fitted model $\widehat{f}()$:

$$
\begin{aligned}
\widehat{y} &= \widehat{f}(\vec{x})\\
\widehat{y} &= \widehat{\beta}_0 + \widehat{\beta}_1x_1\\
\widehat{\mbox{Balance}} &= \widehat{\beta}_0 + \widehat{\beta}_1\mbox{Income}\\
\end{aligned}
$$

But what are the fitted coefficients/parameters $\widehat{\beta}_0$ and $\widehat{\beta}_0$? You can read them from the estimate column from the regression table below, which we generated using the `get_regression_table()` ["wrapper"](https://moderndive.netlify.com/6-regression.html#fig:moderndive-figure-wrapper){target="_blank"} function to extract regression tables as a data frame:

```{r}
model_lm <- lm(Balance ~ Income, data = credit)
model_lm %>% 
  get_regression_table()

# Compute z-axis values
z_grid_2 <- expand.grid(x_grid, y_grid) %>%
  tbl_df() %>%
  rename(
    x_grid = Var1,
    y_grid = Var2
  ) %>%
  mutate(z = coef(model_lm)[1] + coef(model_lm)[2]*x_grid) %>%
  .[["z"]] %>%
  matrix(nrow=length(x_grid)) %>%
  t()
```

In other words, our fitted model $\widehat{f}()$ is:

$$
\begin{aligned}
\widehat{y} &= \widehat{f}(\vec{x})\\
\widehat{y} &= \widehat{\beta}_0 + \widehat{\beta}_1x_1\\
\widehat{\mbox{Balance}} &= \widehat{\beta}_0 + \widehat{\beta}_1\mbox{Income}\\
\widehat{\mbox{Balance}} &= 246.515 + 6.048 \cdot \mbox{Income}
\end{aligned}
$$

Let's view this simple linear regression model in blue and compare it to our naive model $\widehat{f}(\vec{x}) = \widehat{y}$ = \$520.01 in red:


```{r, fig.width=8}
ggplot(credit, aes(x=Income, y=Balance)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "x1 - Income (in $10K)", y = "y - Balance ($)") +
  geom_hline(yintercept = mean(credit$Balance), col = "red", size = 1)
```

It "sort of" seems like the blue regression line fits the points better than our naive model of $\widehat{y}$ = \$520.01. But can we do even better by using a second predictor variable $x_2$ `Limit`



***



# 3. Multiple Regression Model

Let's now fit a multiple linear regression model with two predictors:

$$
\begin{aligned}
y &= f(\vec{x}) + \epsilon\\
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\\
\mbox{Balance} &= \beta_0 + \beta_1\mbox{Income} + \beta_2\mbox{Limit} + \epsilon
\end{aligned}
$$

Kind of like before, we'll estimate/approximate $f()$ with a fitted model $\widehat{f}()$ based on the fitted values of the $\beta$'s from the regression table:

```{r}
model_lm <- lm(Balance ~ Income + Limit, data = credit)
model_lm %>% 
  get_regression_table()

# Compute z-axis values
z_grid_3 <- expand.grid(x_grid, y_grid) %>%
  tbl_df() %>%
  rename(
    x_grid = Var1,
    y_grid = Var2
  ) %>%
  mutate(z = coef(model_lm)[1] + coef(model_lm)[2]*x_grid + coef(model_lm)[3]*y_grid) %>%
  .[["z"]] %>%
  matrix(nrow=length(x_grid)) %>%
  t()
```

Hence:

$$
\begin{aligned}
\widehat{y} &= \widehat{f}(\vec{x})\\
\widehat{y} &= \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2\\
\widehat{\mbox{Balance}} &= \widehat{\beta}_0 + \widehat{\beta}_1\mbox{Income} + \widehat{\beta}_2\mbox{Limit}\\
\widehat{\mbox{Balance}} &= -385.179 - 7.663  \cdot \mbox{Income} + 0.264 \cdot \mbox{Limit}\\
\end{aligned}
$$

Letâ€™s visualize the corresponding regreession plane:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
# base_plot %>%
#   # # Naive model:
#   # add_surface(
#   #   x = x_grid,
#   #   y = y_grid,
#   #   z = z_grid_1
#   # ) %>% 
#   # # Simple linear regression:
#   # add_surface(
#   #   x = x_grid,
#   #   y = y_grid,
#   #   z = z_grid_2
#   # ) %>% 
#   # Multiple regression:
#   add_surface(
#     x = x_grid,
#     y = y_grid,
#     z = z_grid_3
#   )
```


***


# Shrinking $\beta$ Coefficients via LASSO

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, cache=TRUE}
model_formula <- as.formula("Balance ~ Income + Limit")
X <- model.matrix(model_formula, data = credit)[, -1]
y <- credit$Balance
lambda_values <- 10^seq(-3, 10, by=0.05)
model_LASSO <- glmnet(X, y, alpha = 1, lambda = lambda_values)
  coefficients <-
    model_LASSO %>%
    tidy() %>%
    tbl_df() %>%
    # lambda's on x-axis are better viewed on a log-scale:
    mutate(log10_lambda = log10(lambda)) %>%
    select(term, estimate, log10_lambda)
```

**Recall**: 

We now set up a search range of $\lambda$ values to consider in the slider for the Shiny app. Note 
however we don't vary things on a $\lambda$-scale, but rather a $\log_{10}(\lambda)$-scale. Here
is our search range:

* Values of $\log_{10}$ in between $(-3, 10)$
* i.e. Values of $\lambda$ in between $(10^{-3}, 10^{10})$ = (0.001, 10,000,000,000)

**Observe**: As we vary $\log_{10}(\lambda)$, we see that the
LASSO $\beta$ coefficients change. Note the plot does not show how the intercept $\beta_0$ varies, but
the table does.

```{r,echo=FALSE}
inputPanel(
  sliderInput("lambda", label = "log10(lambda)", min = -3, max = 10, value = 1, step = 0.05)
)
```

```{r,echo=FALSE}
renderPlot({
  coefficients %>% 
    filter(term != "(Intercept)") %>% 
    ggplot(aes(x=log10_lambda, y=estimate, col=term)) +
    geom_line() +
    geom_vline(xintercept = input$lambda, linetype="dashed") +
    labs(x="log10(lambda)", y="estimate of coefficient")
})

renderTable({
  coefficients %>% 
    mutate(dist = abs(log10_lambda - input$lambda)) %>% 
    filter(dist == min(dist)) %>% 
    rename(`LASSO Estimate` = estimate) %>% 
    left_join(tidy(model_lm), by="term") %>% 
    rename(`lm Estimate` = estimate) %>% 
    select(term, `LASSO Estimate`, `lm Estimate`)
  
}, rownames = FALSE)
```

**Question**: When we penalize the coefficients very strictly via a high value 
of $\lambda$, note that both slope coefficients $\beta_1$ and $\beta_2$ get
shrunk to 0. What does the value of intercept coefficient $\beta_0$ correspond
to? Hint: We are using no predictor information when $\beta_1=0$ and $\beta_2=0$.





# What Happens at a Particular $\lambda$?

For example at $\log_{10}(\lambda) = 1$, or equivalently at $\lambda = 10^1 = 10$, we have:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, cache=TRUE}
lambda_knob <- 1

coefficents2 <- coefficients %>% 
  filter(log10_lambda == lambda_knob) %>%
  select(term, estimate)

coefficents2 %>% 
  knitr::kable(digits=2)
```

and hence we obtain fitted values using:

$$
\widehat{y} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2\\
\widehat{\mbox{Balance}} = -348.11 - 6.29 \times \mbox{Income} + 0.24\times\mbox{Limit}\\
$$


But where did $\left(\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\beta}_2 
\right) = \left(-348.11, -6.29, 0.24\right)$ come from? How was this computed?
Recall the main optimization problem for the LASSO:

$$
\begin{align}
&\min_{\beta_0, \ldots, \beta_p} \left( \sum_{i=1}^n \left(y_i -\widehat{y}_i\right)^2 + \lambda \sum_{j=1}^p \left|\beta_j\right| \right)\\
\mbox{AKA } &\min_{\beta_0, \ldots, \beta_p} \left( \mbox{RSS} + \mbox{shrinkage penalty} \right)
\end{align}
$$

In other words and in our case, we want to find the $\left(\widehat{\beta}_0, \widehat{\beta}_1,
\widehat{\beta}_2\right)$ combination such that the following value is minimized (let's call it Total):

$$
\mbox{Total} = \sum_{i=1}^{n} \left(\mbox{Balance}_i -\widehat{\mbox{Balance}}_i\right)^2 + \lambda \left( \left|\widehat{\beta}_1\right| + \left|\widehat{\beta}_2\right|\right)\\
$$

where

$$
\widehat{\mbox{Balance}}_i = \widehat{\beta}_0 + \widehat{\beta}_1\mbox{Income}_i + \widehat{\beta}_2\mbox{Limit}_i\\
$$

How does `glmnet()` find these values? It uses *numerical optimization* 
techniques, but they are beyond the scope of this class. If you are interested 
in learning these however, take Prof. Michaela Kubacki's MATH228 Introduction to
Numerical Analysis. We will however do something much clumsier! We will

* Take a bunch of different combinations of coefficients $\left(\widehat{\beta}_0,
\widehat{\beta}_1,  \widehat{\beta}_2\right)$
* Compute Total for each combination
* Plot these values, but with Total on a $\log_{10}$-scale
* Observe which combination of coefficients yields the minimal value of Total

We unfortunately cannot plot how Total varies as a function of all 3 coefficients as this 
is four variables. Let's instead just show how Total varies as a function of the
two slope coefficients $\widehat{\beta}_1$ and $\widehat{\beta}_2$ corresponding
to Income and Limit in an interactive 3D plot:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, cache=TRUE}
coefficients2 <- coefficents2 %>% 
  spread(term, estimate) %>%
  rename(
    beta_Intercept = `(Intercept)`,
    beta_Income = Income,
    beta_Limit = Limit
  )

beta_Income_grid <- seq(from=coefficients2$beta_Income - 100, to=coefficients2$beta_Income + 100, length=200)
beta_Limit_grid <- seq(from=coefficients2$beta_Limit - 2, to=coefficients2$beta_Limit + 2, length=200)

beta_grid <- expand.grid(beta_Income_grid, beta_Limit_grid) %>%
  tbl_df() %>%
  rename(
    beta_Income_grid = Var1,
    beta_Limit_grid = Var2
  ) %>%
  mutate(
    beta_Intercept = coefficients2$beta_Intercept
  ) %>%
  dplyr::select(beta_Intercept, everything())

RSS_vector <- rep(0, nrow(beta_grid))
X <- as.matrix(model.matrix(model_formula, data = credit))
for(i in 1:nrow(beta_grid)){
  beta <- beta_grid[i, ]
  predictions <- X %*% t(as.matrix(beta))
  
  RSS_vector[i] <- credit %>%
    mutate(
      yhat = predictions,
      resid = Balance-yhat
    ) %>%
    summarise(RSS = sum(resid^2)) %>%
    .[["RSS"]]
}

beta_grid <- beta_grid %>%
  mutate(
    RSS = RSS_vector,
    sum_beta_sq = abs(beta_Income_grid) + abs(beta_Limit_grid),
    penalty = sum_beta_sq*lambda_knob,
    total = RSS + penalty
  )
z <- log10(beta_grid$total) %>% matrix(nrow=200) %>% t()

# ggplot(beta_grid, aes(x=beta_Income_grid, y=beta_Limit_grid, z=log(total))) +
#   geom_contour(aes(colour = ..level..)) +
#   geom_vline(xintercept = coefficients2$beta_Income, linetype="dashed") +
#   geom_hline(yintercept = coefficients2$beta_Limit, linetype="dashed") +
#   scale_color_gradient(low="red", high="white") +
#   theme_minimal() +
#   labs(x="Beta Income", y="Beta Limit")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
plot_ly(
  x = as.vector(beta_Income_grid), 
  y = as.vector(beta_Limit_grid), 
  z = z) %>%
  add_surface() %>%
  layout(scene = list(xaxis = list(title = 'Beta1: Income'),
                      yaxis = list(title = 'Beta2: Limit'),
                      zaxis = list(title = 'log10(Total)')))
```

**Question**: For what values of $(\beta_1, \beta_2)$ is Total minimized? Where have we seen this prior?




